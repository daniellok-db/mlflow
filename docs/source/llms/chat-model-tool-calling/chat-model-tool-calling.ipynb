{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a tool-calling model with `mlflow.pyfunc.ChatModel`\n",
    "\n",
    "Welcome to the notebook tutorial on building a simple tool calling model using the [`mlflow.pyfunc.ChatModel`](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatModel) wrapper. ChatModel is a subclass of MLflow's highly customizable [PythonModel](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel), which was specifically designed to make GenAI workflows easier.\n",
    "\n",
    "Briefly, here are some of the benefits of using ChatModel:\n",
    "1. No need to define a complex signature! Chat models often accept complex inputs with many levels of nesting, and this can be cumbersome to define yourself.\n",
    "2. Support for JSON / dict inputs (no need to wrap inputs or convert to Pandas DataFrame)\n",
    "2. Dataclasses for expected inputs / outputs for a better development experience\n",
    "\n",
    "For a more in-depth exploration of ChatModel, please check out the [detailed guide](https://mlflow.org/docs/latest/llms/chat-model-guide/index.html).\n",
    "\n",
    "In this tutorial, we'll be building a simple OpenAI wrapper that makes use of the tool calling support (released in MLflow 2.17.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "First, let's set up the environment. We'll need the OpenAI Python SDK, as well as MLflow >= 2.17.0. We'll also need to set our OpenAI API key in order to use the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow>=2.17.0 openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our model\n",
    "\n",
    "The first step in this tutorial is to define our model by subclassing `mlflow.pyfunc.ChatModel`. This involves defining a `predict()` function that accepts the following arguments:\n",
    "\n",
    "1. `context`: [PythonModelContext](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModelContext) (not used in this tutorial)\n",
    "2. `messages`: List\\[[ChatMessage](https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ChatMessage)\\]. This is the chat input that the model uses for generation.\n",
    "3. `params`: [ChatParams](https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.ChatParams). These are commonly used params used to configure the chat model, e.g. `temperature`, `max_tokens`, etc. This is where the tool specifications can be found.\n",
    "\n",
    "As mentioned earlier, our use-case is fairly simpleâ€”we'll just be forwarding the inputs to OpenAI. However, you can implement any arbitrary logic here for more complex use-cases (e.g. input pre-processing, post-processing for the OpenAI response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ModelInputExample' from 'mlflow.models' (/Users/daniel.lok/miniconda3/envs/dev/lib/python3.9/site-packages/mlflow/models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# replace this with your own MLflow tracking URI\u001b[39;00m\n\u001b[1;32m     13\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:5000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWeatherModel\u001b[39;00m(\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatModel\u001b[49m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, context, messages: List[ChatMessage], params: ChatParams):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# instantiate the OpenAI client\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         client \u001b[38;5;241m=\u001b[39m OpenAI()\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/mlflow/utils/lazy_load.py:41\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m---> 41\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, item)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/mlflow/utils/lazy_load.py:30\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m     32\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py:431\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironment_variables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    426\u001b[0m     _MLFLOW_IN_CAPTURE_MODULE_PROCESS,\n\u001b[1;32m    427\u001b[0m     _MLFLOW_TESTING,\n\u001b[1;32m    428\u001b[0m     MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT,\n\u001b[1;32m    429\u001b[0m )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MlflowException\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model, ModelInputExample, ModelSignature\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependencies_schemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    433\u001b[0m     _clear_dependencies_schemas,\n\u001b[1;32m    434\u001b[0m     _get_dependencies_schemas,\n\u001b[1;32m    435\u001b[0m )\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflavor_backend_registry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_flavor_backend\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ModelInputExample' from 'mlflow.models' (/Users/daniel.lok/miniconda3/envs/dev/lib/python3.9/site-packages/mlflow/models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import mlflow\n",
    "from mlflow.types.llm import (\n",
    "    ChatMessage,\n",
    "    ChatParams,\n",
    "    ChatResponse,\n",
    ")\n",
    "\n",
    "# replace this with your own MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "class WeatherModel(mlflow.pyfunc.ChatModel):\n",
    "    def predict(self, context, messages: List[ChatMessage], params: ChatParams):\n",
    "        # instantiate the OpenAI client\n",
    "        client = OpenAI()\n",
    "\n",
    "        # check if the request contains any tool definitions.\n",
    "        tools = None \n",
    "        if params.tools:\n",
    "            # if so, call the `to_dict` method to convert them to dictionaries\n",
    "            # that can be parsed by the OpenAI SDK. the format of MLflow tools\n",
    "            # are compatible with OpenAI, so no further processing needs to be done.\n",
    "            tools = [tool.to_dict() for tool in params.tools]\n",
    "\n",
    "        # call the API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[m.to_dict() for m in messages],\n",
    "            # pass the tools in the request\n",
    "            tools=tools,\n",
    "        )\n",
    "\n",
    "        # return a ChatResponse, as this is the expected output of the predict method\n",
    "        return ChatResponse.from_dict(response.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the model\n",
    "\n",
    "Next, we need to log the model. This saves the model as an artifact in MLflow Tracking, and allows us to load and serve it later on.\n",
    "\n",
    "(Note: this is a fundamental pattern in MLflow. To learn more, check out the [Quickstart guide](https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html)!)\n",
    "\n",
    "In order to do this, we need to do a few things:\n",
    "\n",
    "1. Define an input example to inform users about the input we expect\n",
    "2. Instantiate the model\n",
    "3. Call `mlflow.pyfunc.log_model()` with the above as arguments\n",
    "\n",
    "Of course, since this is a tool calling tutorial, we'll have to include an example of how to define a tool call. This is handled by instantiating a [FunctionToolDefinition](https://mlflow.org/docs/latest/python_api/mlflow.types.html#mlflow.types.llm.FunctionToolDefinition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.types.llm import (\n",
    "    FunctionToolDefinition,\n",
    "    ParamProperty,\n",
    "    ToolParamsSchema,\n",
    ")\n",
    "\n",
    "# messages to use as input examples\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Please use the provided tools to answer user queries.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in Singapore?\"},\n",
    "]\n",
    "\n",
    "# a sample tool definition. we use the `FunctionToolDefinition`\n",
    "# class to describe the name and expected params for the tool.\n",
    "weather_tool = FunctionToolDefinition(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get weather information\",\n",
    "    parameters=ToolParamsSchema({\n",
    "        \"cities\": ParamProperty(\n",
    "            type=\"string\",\n",
    "            description=\"City name to get weather information for\",\n",
    "        ),\n",
    "    }),\n",
    "# make sure to call `to_tool_definition()` to convert the `FunctionToolDefinition`\n",
    "# to a `ToolDefinition` object. this step is necessary to normalize the data format,\n",
    "# as multiple types of tools (besides just functions) might be available in the future.\n",
    ").to_tool_definition()\n",
    "\n",
    "# the full input to the model includes both the messages and tools. see the\n",
    "# docs for `mlflow.types.llm.ChatRequest` for full details on the input format.\n",
    "input_example = {\n",
    "    \"messages\": messages,\n",
    "    \"tools\": [weather_tool],\n",
    "}\n",
    "\n",
    "# instantiate the model\n",
    "model = WeatherModel()\n",
    "\n",
    "# log the model\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"weather-model\",\n",
    "        python_model=model,\n",
    "        input_example=input_example,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model for generations\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
